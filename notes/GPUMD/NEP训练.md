
# NEP训练

使用 GPUMD 训练 NEP 需要准备名为 `nep.in` 的输入文件、训练集 `train.xyz`、测试集 `test.xyz`（可选）和 GPUMD 提交脚本 `rungpumd.pbs`。
`nep.in` 中包含各种与 NEP 训练相关的超参数。
几乎所有超参数都有合理的默认值，常用超参数默认值如下：

> `version 4`
> `prediction 0` # 0 是训练模式，1 是预测模式
> `type <number_of_species> [<species>]` # 根据 .xyz 文件来设置
> `cutoff 8 4` # 径向描述符截断 8 Å，角度描述符截断 4 Å
> `n_max 4 4` # 径向描述符包含 4 + 1 = 5 个径向函数，角度描述符包含 4 + 1 = 5 个径向函数
> `basis_size 8 8` # 径向描述符包含 8 + 1 = 9 个径向基函数，角度描述符包含 8 + 1 = 9 个径向基函数
> `l_max 4 2 0` # 使用三体、四体角度描述符，不使用五体角度描述符
> `neuron 30` # 在隐藏层使用 30 个神经元
> `lambda_1 <weight>` # L1 正则 默认设置为 $\sqrt{N} / 1000$ ，其中 $N$ 是训练参数的总数
> `lambda_2 <weight>` # L2 正则 默认设置为 $\sqrt{N} / 1000$ ，其中 $N$ 是训练参数的总数
> `lambda_e 1` # 损失函数中能量权重为 1.0
> `lambda_f 1` # 损失函数中力的权重为 1
> `lambda_v 0.1` # 损失函数中位力权重为 0.1
> `batch 1000` # 训练的每一步使用训练集中的 1000 个结构
> `population 50` # 种群大小为 50
> `generation 100000` # 训练步数为 100000 步

NEP 使用多体描述符，力程是截断半径的两倍。截断半径极大地影响势函数的精度，因此需要谨慎地测试。
一般而言，共价键主导的体系需要较大的角度描述符截断半径，且不需要过大的径向描述符截断半径，所以 `cutoff 5 5` 或 `cutoff 6 4` 即可；
默认的 `cutoff 8 4` 更适合离子晶体。


下面介绍一些个人训练 NEP 的经验：

1. 可以使用多个 GPU 来加速 NEP 训练，即在 PBS 提交脚本中设置 `export CUDA_VISIBLE_DEVICES=0,1`。
2. 训练 NEP 时，如果训练集构型较多，可以适当增大神经元数目。
	如果训练集构型不够丰富，可能会观察到能量、原子受力、位力都接近收敛时，L1、L2 还有明显的下降趋势。
	如果发现 `nep.txt` 中很多参数趋近于 0，这代表有的神经元被杀死了。
3. 为加速 NEP 训练，在主动学习时不妨分两步走。
	第一步，设置 `lambda_e 0.1`、`lambda_f 1`、`lambda_v 0.1`，`batch <N_train / 10>`，训练十万步确保力接近收敛；
	第二步，设置 `lambda_e 1.0`、`lambda_f 1`、`lambda_v 0.1`，`batch <N_train>`，再训练十万步即可确保能量、力均收敛。
4. NEP 每次训练时，不妨把测试误差小的结构扔在测试集，以确保需要的构型保留在训练集中，同时有一个测试集来避免模型过拟合。

一般来说，测试势函数的稳定性时，主要就看：
1. 参数分布是否均匀；
2. 跑长时间的 MLMD（例如 10 ns）会不会崩溃，以及均匀抽帧出来的结构 DFT 计算的能量和 NEP 预测能量是否接近；
3. 对比相关物理性质如 E-V 曲线、声子色散等。
注意，机器学习势拟合的最终目标不是非常小的 RMSE，而是能落实到具体的物理量，这才是最终目标。
所以，RMSE 降不下去不一定代表这个势函数训练得不好。
如果非要追求特别小的 RMSE，由于 RMSE 会放大个别大的误差，因此可以手动删去误差最大的几个结构，但这么做的合理性显然是有待商榷的。

目前已有众多训练好的 NEP 势函数模型，它们大部分搜集在 [樊老师的仓库](https://gitlab.com/brucefan1983/nep-data) 中。
